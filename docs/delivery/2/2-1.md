#### Task 2-1: Implement Run Simulation API Command

##### Description

Implement the API endpoint that starts the model simulation. This corresponds to a `call` (or `update`) action on the `simulation` resource. It should parse any provided simulation parameters, trigger the simulation run through SageModeler’s internal APIs, and return an immediate response. It also needs to coordinate with the event broadcasting system to signal simulation start/completion, and with the busy-flag system to prevent overlap.

##### Status History

| Timestamp | Event Type | From Status | To Status | Details | User |
| :---- | :---- | :---- | :---- | :---- | :---- |
| 2025-06-22 17:38:00 | Created | N/A | Proposed | Task drafted (PBI-2) | Chad |

##### Requirements

* Recognize a simulation run command: likely `action:"call", resource:"simulation"` (or `resource:"simulation/run"` if we had sub-resource, but simpler just "simulation"). We will handle both for flexibility if needed (or document one).  
    
* Check no simulation is currently running: if a run is in progress (we can track via a flag as described or by checking `SimulationStore.modelIsRunning` if accessible), then do not start another. Instead respond with `success:false, error:"Simulation already running"`.  
    
* Parse optional parameters in `message.values`:  
    
  * `duration` (number of steps or time units to run). This is typically an integer. We should validate it (\>=1, and perhaps \<= some max like 5000 as code uses \[27†L153-L161\], they clamp between 1 and 5000). We can mimic that clamp or simply validate 1 \<= duration \<= 5000\. If outside, either clamp or error. Possibly safer to clamp to allowed range as UI does (GraphStore.onSetDuration does `Math.max(1, Math.min(n, 5000))`). We can do the same (clamp without error).  
  * `timeUnit` or `stepUnits` if provided (like "seconds", "years", etc., corresponding to SimulationStore.stepUnitsName maybe). That might be advanced; we can support if easy: SimulationActions.setStepUnits expects a TimeUnits object or something. Actually SimulationStore.onSetStepUnits expects a unit object with `.unit` property and a `hasCollectors` flag. This is complex to manage externally (the units mapping is internal). We might skip direct support for changing time units via API unless needed. If plugin needs it, they could conceivably include an index or id for unit. For now, likely not needed. We'll ignore `timeUnit` param or have minimal support if easy: perhaps allow specifying the string name ("Month", "Year") and find corresponding unit constant via `TimeUnits.units`. But that's extra. Possibly skip or do if trivial.  
  * Other possible parameters: perhaps a boolean to control data recording (if there's an option to not record data points). There's `SimulationActions.recordOne`, `recordStream` in SimulationStore, but those are for manual recording. The normal run always records to data context if CODAP is connected. We'll not expose those toggles via API now.  
  * If none provided, use current simulation settings as is.


* Set simulation parameters:  
    
  * If `duration` provided, call `SimulationActions.setDuration(duration)`. This updates SimulationStore.settings.duration (clamped inside store as needed). That triggers no immediate run, just sets.  
  * If `timeUnit` provided and we decide to support: find the matching unit in `TimeUnits.units` list (which likely is an array of units configured). There is `SimulationStore.onSetStepUnits(unit, hasCollectors)` requiring a unit object. The TimeUnits class might have a helper. Possibly easier: the simulation settings in model JSON has `settings.simulation.stepUnits` which is likely an integer code for the unit. We might not want to delve deep. We'll skip it for now for simplicity, or treat if given but not implemented, we respond error "Unsupported parameter". Let's do that: only support `duration` explicitly.


* Trigger run: Call `SimulationActions.runSimulation()` to start the simulation.  
    
  * Note: Because Reflux actions are synchronous (as defined with `{sync:true}` for runSimulation), this call will synchronously begin the simulation steps in the SimulationStore. If it is a static model simulation, it might actually complete very quickly, possibly before we even send a response. If it's time-based, `_runSimulation` might schedule asynchronous steps. In either case, we likely want to return response immediately. We should actually likely call runSimulation and not wait for it fully. But since it's sync, our code will actually only regain control after runSimulation finishes or schedules asynchronous parts. For static mode, by the time our function returns, simulation might have ended (which is fine, we'll still return success). For time-based, runSimulation will schedule things and return quickly.  
  * We might consider calling runSimulation in a setTimeout(0) to decouple it from immediate execution, allowing us to return faster. But probably not needed. It's okay if our method spends up to e.g. tens of ms doing these calls. It should still return promptly (under 200ms typically) unless model is huge. The static solve is pretty quick.  
  * So we'll call it directly.


* Immediately after triggering, send back a response: `success:true`. We may include in data something like `started:true` or the simulation’s configured duration or an ID. Not much to include; success implies started. Possibly include a requestId or run id if we want to track multiple runs, but likely unnecessary.  
    
* Mark the system as busy with simulation: set our `sageApi.simulationRunningFlag = true`. We will clear this when simulation ends (which we detect via event broadcasting in PBI-3 tasks – our event handler for simulationCompleted can clear it). If we implement busy guard in PBI-6, that might cover it globally. But for now, do the flag here. If event system is not yet implemented, we might set a timeout to clear it after a certain period (if we had to, but events will handle precisely when it ends). We'll integrate with events: simulationStarted event triggers the flag, simulationCompleted clears it. For immediate, since we are directly calling run now, we can preemptively set it just before or after calling run. Actually, better to set it before calling runSimulation (so if any API call triggers during, they see busy true). The simulationStarted event (when we catch it) is effectively simultaneous; but setting beforehand is fine. We'll do: `busy=true` then call run.  
    
* If any error occurs (for example, if no SimulationStore or something – not likely – or if run cannot start because model not runnable), how to detect? `SimulationActions.runSimulation` itself doesn’t return a value or throw normally. But SimulationStore only runs if `modelIsRunnable` is true (it checks in `_runSimulation()`: if modelIsRunnable false, it likely won't do anything). If model is not runnable (like if no nodes that produce output), UI usually greys out Run button. We might simulate pressing it; if not runnable, likely nothing happens. We should handle that: We can check `SimulationStore.settings.modelIsRunnable` before running (store has that flag in settings). If false, we respond with error "Model is not runnable". Alternatively, we could still call it and nothing will happen; better to inform caller. We can access `AppSettingsStore.modelIsRunnable` or `SimulationStore.settings.modelIsRunnable`. Actually, GraphStore has `updateModelIsRunnable()` it calls whenever graph changes to set SimulationStore.settings.modelIsRunnable. Might be simplest to check `SimulationStore.settings.modelIsRunnable` if possible. Let's see: SimulationStore.settings (which we made in simulation-store interface \[27†L55-L63\]) has modelIsRunnable property. SimulationStore is a Reflux store; maybe we can import `SimulationStore` (like we do GraphStore) and access a property or static. Possibly not directly. Alternatively, GraphStore has a method `getMinimumSimulationType()` or a property somewhere to determine if runnable. But that might be complicated to call. Possibly easier: attempt run and if model not runnable, SimulationStore likely triggers simulationEnded immediately or doesn’t trigger start at all.  
    
  * For robust feedback, let's attempt to determine if model is runnable: A model is runnable if there's at least one causal link chain or a way to produce data. If no links and all nodes are independent, the model might still run (but produce trivial output). Actually, SageModeler might consider model with no dynamic variables as not runnable.  
  * Perhaps simpler: after calling SimulationActions.runSimulation, check if it did anything. But how? It's asynchronous in part.  
  * Possibly do: if \!modelIsRunnable, return error. To get modelIsRunnable, maybe use GraphStore or SimulationStore. Looking at GraphStore.updateListeners at \[43†L1466-L1473\], it compares lastRunModel and triggers SimulationActions.runSimulation if model changed in some cases (auto-run?), not directly giving modelIsRunnable. In GraphStore.init it sets modelIsRunnable false initially and updates it via GraphStore.\_updateModelIsRunnable (which likely sets if at least one node). I'd hazard that modelIsRunnable is true if there's at least one node (?), or maybe if any formula present. In any case, if user can press Run or not is the indicator. Without UI, an empty model should not run. Let's do: if GraphStore.getNodes().length \== 0 (no nodes), treat not runnable. If there are nodes but perhaps no links, still could run (each node might just output initial value as data – likely they still produce a data point). Actually, in an all independent nodes scenario, not sure if SageModeler records data – but likely it does record each node’s value as a result for one time step. So run might be considered valid even with no links. So perhaps everything except zero nodes is runnable.  
  * We'll implement a basic check: if no nodes, error "No model to run". If yes nodes, proceed. This should cover obvious case.


* Ensure the simulation triggers events: we assume our events broadcasting tasks (PBI-3) will catch SimulationActions.simulationStarted/Ended and broadcast accordingly. We don't need to do extra for events here except ensure they fire:  
    
  * SimulationStore itself triggers simulationStarted action? Not clearly shown, but maybe expanding panel triggers simulationStarted automatically. If not, we can manually dispatch SimulationActions.simulationStarted ourselves just before run if needed to generate event. But likely not needed, because event broadcasting can listen for simulationEnded (we saw simulationEnded is triggered at end) and simulationFramesCreated. Possibly SimulationStore doesn’t explicitly broadcast a "started" event. But the PRD expects a simulationStarted event for API. Perhaps we should manually emit an API event for started right when run is invoked. But to be consistent, we may implement simulationStarted event by capturing when runSimulation is called. Actually, perhaps we should explicitly broadcast simulationStarted in our API handler as soon as we trigger run. But the PRD implies simulationStarted is emitted from SageModeler side not from plugin, ideally when simulation truly begins.  
  * If the simulation has any slight delay, maybe it sets modelIsRunning true first, we could consider that the start. Maybe simpler: we will emit simulationStarted event essentially immediately after calling runSimulation (maybe after a short delay to allow internal state change, but likely immediate is fine). Our event broadcasting system (Task 3-3 maybe) could do it like: either listen on `SimulationActions.simulationEnded` and also simulationStarted if exists. There is no simulationStarted action in SimulationActions (except an entry in list \[27†L35-L39\], actually yes it exists as a defined action at \[27†L35-L39\] "simulationStarted"). Possibly SimulationStore calls `SimulationActions.simulationStarted()` when simulation begins. It's not visible in the snippet, but maybe in `_runSimulation` it does at start or within expandSimulationPanel or collapse. Actually, SimulationStore.onExpandSimulationPanel sets modelIsRunning=true and then calls `_runSimulation()`. It does not explicitly call simulationStarted action. However, in UI, expanding panel might cause an event (maybe SimulationActions.simulationStarted is fired somewhere else). Possibly oversight. But codapConnect listens for simulationEnded only, not simulationStarted. So maybe simulationStarted action isn't used except to update UI state maybe.  
  * For our API events, we do want a started event. We can manually fire one in our event broadcast system right after runSimulation is triggered (like broadcast "simulationStarted"). Or set up a listener on modelIsRunning property changes. Simpler: We can broadcast simulationStarted from within handleRunSimulation (this task) after successfully invoking run, since at that point simulation effectively started. But doing it here might circumvent the event broadcasting centralized logic. Better to let event system handle it consistently: e.g. event system can tie into the `simulationRunningFlag` or the runSimulation call.  
  * Perhaps for simplicity, in our implementation we won't explicitly send events here; we'll rely on PBI-3 tasks to either intercept run action or use a small hack: maybe in our events tasks, we hook SimulationActions.runSimulation as well to broadcast started. Alternatively, just broadcast started immediately when the user calls run (less ideal).  
  * We'll coordinate it in PBI-3; mention that simulationStarted will be emitted essentially at the same time. So, here focus on run itself.

##### Implementation Plan

1. **Identify command**: In the API message dispatcher, map `action:"call"` with `resource:"simulation"` (and possibly also accept `action:"update"` for simulation) to this handler.  
     
2. **Check busy state**: If a global busy flag (like `sageApi.isBusy` or specifically `sageApi.simulationInProgress`) is true, return error `{"error":"Simulation already in progress", success:false}`. Also, if any simulation is running as per our internal flag or `GraphStore`/`SimulationStore` state. Use `sageApi.simulationRunning` as maintained by events, or if not set up, perhaps check if `GraphStore` has any indicator (GraphStore doesn't track running directly). Alternatively, if we call this and modelIsRunning is true, it means a simulation is currently active (like maybe user triggered one concurrently). SimulationStore.modelIsRunning is tough to get from outside, but maybe `SimulationStore.settings.modelIsRunning` could be accessible if we import `SimulationStore` as a Reflux store instance. That might not be straightforward unless the store exports an object. Instead, rely on our own tracking (since events will set a flag on simulationStarted). We'll integrate with events: assume if an event for simulationStarted hasn't had a matching completed, our internal flag is on. So check `sageApi.simulationRunningFlag`. If true, block.  
     
3. **Check model presence**: If `GraphStore.getNodes().length == 0`, return error `{"error":"No model loaded", success:false}` because nothing to simulate.  
     
4. **Optional: Check model is runnable**: If we could easily detect an unrunnable scenario (like all nodes have no initial values or something?), that’s complex. We skip explicit, or we could do minimal: if all nodes are qualitative with no values? Hard. We'll trust the above presence check. Possibly if GraphStore.allLinksAreUndefined() returns true (meaning no defined relationships?), but even then you can still run static and just get initial values. So not necessary. We'll skip additional.  
     
5. **Parse parameters**:  
     
   * If `message.values.duration` present: ensure it's a number. If not a valid number, return error `"Invalid duration"`. If number, clamp between 1 and 5000 (like `duration = Math.max(1, Math.min(duration, 5000))`).  
   * If `message.values.duration` was clamped, we could use the clamped value.  
   * If `message.values.timeUnit` present (say as string "month"): We decide to **not implement** for now due to complexity. We can simply ignore it or return an error/warning "timeUnit parameter not supported". Better to ignore to not fail an otherwise fine request. But if user sends it expecting effect, ignoring might be confusing. The PRD didn't explicitly mention controlling time unit, just "optional parameters" generically. Possibly they meant duration primarily. We'll implement **duration** and not mention time unit. So if we see `values.timeUnit`, either do nothing or error. I'll choose to do nothing (quietly ignore) to still run sim with default units. Document in dev notes that timeUnit not supported yet.  
   * If any other unknown keys in values, ignore them (or we could warn in log).

   

6. **Apply parameters**:  
     
   * If we have a valid `duration` number, call `SimulationActions.setDuration(duration)`. This will synchronously update SimulationStore. We might want to yield for a moment to let any listeners update modelIsRunnable or UI. But probably fine; it triggers notifyChange in store, but we can proceed. Actually, SimulationStore.onSetDuration calls notifyChange synchronously and returns. No problem.  
   * (If we were to set step units, would call `SimulationActions.setStepUnits(unitObj)` similarly. But skipping.)

   

7. **Set busy flag**: Mark our internal `sageApi.simulationRunningFlag = true`. Also perhaps set a global `sageApi.busy = true` because while simulation runs, we consider the system busy to block other writes (PBI-6). But likely we'll manage global busy in PBI-6 tasks by checking simulationRunning. For now set specific simulationRunning.  
     
8. **Start simulation**: call `SimulationActions.runSimulation()` from the imported actions. This will initiate the simulation.  
     
   * After this call returns, either simulation completed (if it was quick static run) or it's in progress (if dynamic). Either way, from API perspective, we consider it started successfully if no exception thrown.  
   * If an exception thrown here (unlikely, but let's say simulation couldn’t start due to some internal error), catch it. Then set `simulationRunningFlag` back to false (since it didn't start), and return error "Failed to start simulation". But we expect none.

   

9. **Respond**: Construct a response: `success:true`. Could include maybe `{"status":"started"}` or the used duration. Possibly we include `"duration": actualDuration` in data (the one we set, which might be clamped). That could confirm to plugin how long it will run. We can do that.  
     
   * E.g., `data:{ duration: SimulationStore.settings.duration }`. But accessing that after run might require reading SimulationStore (we can get the value we used, or after calling setDuration, use the same value). The difference is if we clamped input, we want to let them know the adjusted value. We have that value in our variable. So do that: `data:{ duration: effectiveDuration }`.  
   * Not strictly necessary for functionality, but a nice explicit confirmation.  
   * If no parameters given, we could include the default duration currently set, just for completeness. That we can get via `SimulationStore.settings.duration` if accessible. Or GraphStore has `settings.simulation.duration` in model JSON – but accessing that is similar challenge. It's easier: after calling setDuration or if none given, we could find the current duration. Actually, we can call `AppSettingsStore` or `SimulationStore`.  
   * Perhaps skip if no param; or we can not include any data except maybe a message string "Simulation started". Up to us. I'll include duration if we either set it or if we can easily get it. Maybe not bother if not easily accessible.  
   * We'll do: if we parsed a duration param, include that (maybe after clamping). If none, data can be empty or exclude.

   

10. If at any point before triggering run we encountered an error, send error and do not call runSimulation.  
      
11. Integrate with events: We assume separate tasks will pick up that simulationCompleted event and then call e.g. `sageApi.simulationRunningFlag=false`. But in case of static simulation finishing almost instantly (within the same event loop call, possibly before we send response?), our flag might be set to false quickly. That’s fine. The response is already constructed and likely unaffected.  
      
12. Also note: If simulation runs asynchronously (dynamic mode), our flag remains true until events clear it. If static mode, simulation might mark ended before our function returns. But since we set flag true before, we might need to immediately clear it because it finished. However, detecting that is tricky in sync code. We could potentially detect that by checking `SimulationStore.settings.modelIsRunning` after runSimulation returns. If it's false, that means it already ended. So we could decide to clear our flag in that scenario (meaning simulation ended during this call). But simulationEnded event will also come (maybe even before our code continues? Or after, depending if they dispatch events asynchronously via Reflux). It's complicated but not critical: if a static simulation ended right away, our `simulationRunningFlag` might be left true erroneously for a brief time until simulationCompleted event is processed (which might be next tick). We could avoid that by checking `GraphStore.linkKeys` or something or `SimulationStore.settings.modelIsRunning`. Possibly do: after runSimulation, if `GraphStore` or internal state says it's no longer running, then we set our flag false immediately (meaning it finished within this call). How to check: maybe check `sageApi.simulationRunningFlag` itself – but we set it true. Check `SimulationStore.settings.modelIsRunning` if accessible (should be false if ended). We can attempt to get that via something like `AppSettingsStore.modelIsRunning` if exists. Actually, SimulationStore has modelIsRunning in settings. To access, if SimulationStore is a Reflux store, we might import `SimulationStore` from its module which could export e.g. `SimulationStore` object or an instance. Possibly the stores in that older codebase might not export easily. Another way: GraphStore (which we have) might have a property `GraphStore.nodes` where each node might have frames recorded if simulation runs. But not straightforward to detect ended. Perhaps skip this nuance; the slight discrepancy is minor (in worst case, a static sim completed but we still think running for a few milliseconds until events catch up). This could block an immediate subsequent command in the same tick unnecessarily, but that's an edge case. Acceptable. So I'll not complicate with immediate end check.  
      
13. That covers the main flow.

##### Verification

* **Unit Test**:  
    
  * Create a simple model (like one node with an initial value). Then call runSimulation API. Expect response success. Check that:  
      
    * The simulationRunning flag (if exposed via API internal, not directly to user though) would have toggled true then false eventually. But we can indirectly test by trying to call a second run immediately in the test and expecting an error (if the first is still flagged running). For static simulation, since it ends quickly, by the time we call second run perhaps it's flagged ended, so second might succeed. Hard to test concurrency exactly in a unit test environment without actual timing. Possibly skip that at unit and trust integration for concurrency.  
    * However, we can simulate concurrency by artificially setting the flag true and calling again to see it errors. Do that: set `sageApi.simulationRunningFlag=true` (simulate that a sim is running), then call handleRunSimulation and confirm it returns error about already running. Then set flag false and call, confirm success. This ensures the busy check works.  
    * Test that after calling runSimulation, the SimulationStore likely recorded something: for static simulation, GraphStore each node might have `node.value` updated or frames recorded. Actually, static one might produce one frame in CODAP. In our environment without CODAP, GraphStore will still populate node.frames maybe. If we inspect `GraphStore.getNodes()[0].frames`, it might now have an array of values from run (depending if recordOne was triggered). Actually, need to see how static simulation results are stored: Possibly they directly push to CODAP via codapConnect instead of storing frames. For test environment with no CODAP, maybe SimulationActions.simulationFramesCreated triggers GraphStore.updateSimulationData (which populates Node.frames arrays). Yes, GraphStore.updateSimulationData loops and pushes values into node.frames. But static sim might not produce frames? Or maybe it still uses one frame. Possibly not, static might bypass data recording except final values. Hard to ascertain quickly, but maybe a node's `node.value` property might reflect new calculation if any formula. For a single node model, value remains same, so nothing to check. For a link model, maybe one node calculates something. That level of test might be too deep. It's more integration.

    

  * Test parameter: set a custom duration. E.g., if model is time-based (like has accumulator?), we could set a short duration. We may not have such model in unit test easily. But we can at least verify that calling runSimulation with duration in values calls SimulationActions.setDuration with clamping by verifying SimulationStore.settings (if we could spy on SimulationActions or SimulationStore). Alternatively, we can monkey patch SimulationActions.setDuration to capture the input or observe GraphStore which might record duration somewhere. Actually, `GraphStore.settings.simulation.duration` could reflect it, or SimulationStore.settings. Without easy access, maybe trust. We can try hooking `SimulationActions.setDuration.listen()` for test to record the value passed. That might be complicated. Possibly skip verifying clamping logic beyond a straightforward scenario: e.g., send duration 0, expecting it to clamp to 1\. We could then get SimulationStore.settings.duration (if we hack to get SimulationStore from require), but it's heavy. Instead, we could simulate by calling our handler with 0, and then check the response data if it included the duration (if we decide to include it). If we included duration in success data, it should show 1\. That might be a way: in handleRunSimulation, we planned to include effective duration if provided. So test sending 0 \-\> response success, data.duration \= 1\. If so, it clamped. Similarly send 6000 \-\> expect data.duration \= 5000\. So that tests clamping indirectly. Good approach.  
      
    * We'll implement response data including effective duration if param was given. Then test these boundary conditions.

    

  * Test invalid scenario: if no model loaded (GraphStore empty). Ensure run returns error "No model loaded".  
      
  * If we can simulate model not runnable (not sure how to easily simulate that state except an empty model which we did), but maybe a model with no relationships is still runnable. Possibly skip.


* **Integration Test**:  
    
  * In a full environment with CODAP and SageModeler, use a plugin to call runSimulation when a model is present. Check:  
      
    * The simulation indeed runs: e.g., if CODAP’s data table receives new data rows (if we have access, open data table and count rows after run).  
    * The plugin receives a `simulationStarted` event right away and a `simulationCompleted` event after appropriate time (for dynamic, a small delay, for static almost immediately).  
    * If they provided a duration param, verify that the simulation ran for that many steps (maybe by number of rows in CODAP table or final experiment number). For example, if original duration was 20, and plugin runs with 10, check data table has 10 rows and the internal setting is now 10\.  
    * Test concurrency: trigger run via API, then quickly trigger another run (maybe via a second API call or UI click) before the first completes. Ensure the second attempt was blocked (the plugin should get an error response for second call). If from UI, perhaps the Run button is disabled anyway when running, so not a real scenario. But plugin might try twice.  
    * Possibly test a scenario with simulation still running and plugin tries to modify model (like add node). That should be blocked by busy flag (in PBI-6 tasks). We'll test that in PBI-6 integration.  
    * Also test calling run with no model (should error as unit test).

    

  * If possible, test that after run, the model’s state updated (like experiment number incremented by 1 for a static run – in SimulationStore there's experimentNumber field incremented on each run). But verifying that would require capturing internal state or checking if another run yields cumulative data. Slightly advanced, maybe skip.

##### Files Modified

* `src/code/sage-api.ts` – Implement `handleRunSimulation` or similar. Import `SimulationActions` from `../stores/simulation-store` if available (the code structure likely exports actions as in SimulationStore we saw Reflux.createActions so perhaps it exports SimulationActions). If not easily importable, we might have to require the simulation-store module and access `SimulationActions` from it (assuming it exports). We will manage that with either an import or a dynamic require.  
* Possibly also import `SimulationStore` or `AppSettingsStore` to access modelIsRunnable or default settings if needed. But might skip if not needed.  
* Should ensure to import any needed classes for param validation (like the constant 5000 maybe not needed, we can just use number). Possibly not.  
* No changes to simulation-store code itself, just using it.  
* In PBI-6, we might update this code to integrate global busy logic (like setting a global busy flag and using it).